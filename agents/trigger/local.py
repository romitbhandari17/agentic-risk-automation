import os
import json
import uuid
import logging
from typing import Any, Dict

import boto3

# Also import local ingestion and risk handlers so trigger can call them directly when running locally
try:
    from agents.ingestion import main as ingestion_main
except Exception:
    ingestion_main = None

try:
    from agents.risk_analysis import main as risk_main
except Exception:
    risk_main = None

# Configure logging
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# Helper: try to load terraform outputs JSON (preferred) or dev auto tfvars
def _load_tf_outputs() -> Dict[str, str]:
    """Load infra/envs/dev/terraform_outputs.json if present (generated by `terraform output -json`).

    Returns a flat map of outputs to their primitive values where possible.
    """
    outputs_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'infra', 'envs', 'dev', 'terraform_outputs.json')
    if not os.path.exists(outputs_path):
        return {}

    try:
        with open(outputs_path, 'r') as f:
            data = json.load(f)
        # terraform output -json yields {"name": {"value": ..., "type": ...}, ...}
        flat: Dict[str, str] = {}
        for k, v in data.items():
            val = v.get('value') if isinstance(v, dict) else v
            # If it's a list/dict, stringify it; otherwise use string value
            if isinstance(val, (list, dict)):
                flat[k] = json.dumps(val)
            elif val is None:
                flat[k] = ''
            else:
                flat[k] = str(val)
        return flat
    except Exception as e:
        logger.warning(f"Failed to load terraform outputs JSON at {outputs_path}: {e}")
        return {}

# Helper: try to load terraform dev auto tfvars for local overrides
def _load_dev_tfvars() -> Dict[str, str]:
    """Parse infra/envs/dev/dev.auto.tfvars (simple key = value parser).

    Returns a dict of string keys to string values. Ignores comments and blank lines.
    """
    tfvars_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'infra', 'envs', 'dev', 'dev.auto.tfvars')
    result: Dict[str, str] = {}
    if not os.path.exists(tfvars_path):
        return result

    try:
        with open(tfvars_path, 'r') as f:
            for raw in f:
                line = raw.strip()
                if not line or line.startswith('#'):
                    continue
                # Expect lines like: key = "value" or key = 'value' or key = value
                if '=' not in line:
                    continue
                parts = line.split('=', 1)
                key = parts[0].strip()
                val = parts[1].strip()
                # Strip optional trailing comments
                if '#' in val:
                    val = val.split('#', 1)[0].strip()
                # Remove surrounding quotes if present
                if (val.startswith('"') and val.endswith('"')) or (val.startswith("'") and val.endswith("'")):
                    val = val[1:-1]
                result[key] = val
    except Exception as e:
        logger.warning(f"Failed to read tfvars at {tfvars_path}: {e}")
    return result

# Load terraform outputs first (preferred), then tfvars
_tf_outputs = _load_tf_outputs()
_tfvars = _load_dev_tfvars() if not _tf_outputs else {}

# Environment variables and overrides (tf outputs > tfvars > env > default)
AWS_REGION = _tf_outputs.get('region') or _tfvars.get('region') or os.environ.get("AWS_REGION", "us-east-1")
STATE_MACHINE_ARN = _tf_outputs.get('state_machine_arn') or _tfvars.get('state_machine_arn') or os.environ.get("STATE_MACHINE_ARN", "arn:aws:states:us-east-1:968239734180:stateMachine:agentic-risk-automation-dev-contract-review")
INGESTION_LAMBDA_ARN = _tf_outputs.get('ingestion_lambda_arn') or _tfvars.get('ingestion_lambda_arn') or os.environ.get("INGESTION_LAMBDA_ARN")
RISK_ANALYSIS_LAMBDA_ARN = _tf_outputs.get('risk_analysis_lambda_arn') or _tfvars.get('risk_analysis_lambda_arn') or os.environ.get("RISK_ANALYSIS_LAMBDA_ARN")
INGESTION_ZIP_PATH = _tf_outputs.get('ingestion_zip_path') or _tfvars.get('ingestion_zip_path') or os.environ.get('INGESTION_ZIP_PATH')
RISK_ZIP_PATH = _tf_outputs.get('risk_analysis_zip_path') or _tfvars.get('risk_analysis_zip_path') or os.environ.get('RISK_ZIP_PATH')

step_functions = boto3.client("stepfunctions", region_name=AWS_REGION)


def handler(event: Dict[str, Any], context: Any) -> Dict[str, Any]:
    """
    Trigger Lambda for initiating Step Functions workflow.

    Can be invoked by:
    1. S3 Event Notification
    2. EventBridge Rule
    3. Direct Lambda invocation
    4. API Gateway

    Input formats:

    # Direct invocation:
    {
      "s3": {
        "bucket": "my-bucket",
        "key": "contracts/contract.pdf"
      },
      "vendor_metadata": {
        "region": "us-east-1",
        "contract_type": "MSA"
      }
    }

    # S3 Event Notification:
    {
      "Records": [{
        "s3": {
          "bucket": {"name": "..."},
          "object": {"key": "..."}
        }
      }]
    }
    """

    # For local development default event (can be overridden by incoming event)
    event = event or {
        "s3": {
            "bucket": "agentic-risk-automation-dev-artifacts",
            "key": "contracts/contract.pdf"
        },
        "vendor_metadata": {
            "region": "us-east-1",
            "contract_type": "MSA"
        }
    }

    logger.info(f"[handler] Trigger Lambda invoked with event keys: {list(event.keys())}")

    # Parse S3 event
    if "Records" in event and event["Records"]:
        # S3 event notification format
        record = event["Records"][0]
        bucket = record["s3"]["bucket"]["name"]
        key = record["s3"]["object"]["key"]

        vendor_metadata = {
            "region": "us-east-1",
            "contract_type": "MSA",
            "source": "s3-event"
        }

        logger.info(f"[handler] Parsed S3 event: s3://{bucket}/{key}")
    elif "s3" in event:
        # Direct format
        s3_field = event["s3"]
        # allow either dict with bucket/key or direct values
        if isinstance(s3_field, dict) and "bucket" in s3_field and "key" in s3_field:
            bucket = s3_field["bucket"]
            key = s3_field["key"]
        else:
            # assume legacy flat structure
            bucket = event["s3"].get("bucket") if isinstance(event["s3"], dict) else event["s3"]
            key = event["s3"].get("key") if isinstance(event["s3"], dict) else None

        vendor_metadata = event.get("vendor_metadata", {
            "region": "us-east-1",
            "contract_type": "MSA"
        })

        logger.info(f"[handler] Parsed direct invocation: s3://{bucket}/{key}")
    else:
        error_msg = "Invalid event format. Expected S3 event or direct format with 's3' key"
        logger.error(f"[handler] {error_msg}")
        raise ValueError(error_msg)

    # Generate contract ID if not provided
    contract_id = event.get("contract_id") or str(uuid.uuid4())

    # Prepare Step Functions input
    sfn_input = {
        "s3": {
            "bucket": bucket,
            "key": key
        },
        "vendor_metadata": vendor_metadata,
        "contract_id": contract_id
    }

    # Call ingestion handler locally (when available) to get extracted clauses and contract_id
    ingestion_result = None
    if ingestion_main and hasattr(ingestion_main, "handler"):
        try:
            logger.info("[handler] Invoking local ingestion handler")
            ingestion_result = ingestion_main.handler(sfn_input, None)
            logger.info(f"[handler] Ingestion returned keys: {list(ingestion_result.keys()) if isinstance(ingestion_result, dict) else 'N/A'}")
            # prefer contract_id returned by ingestion if present
            contract_id = ingestion_result.get("contract_id") or contract_id
            extracted = ingestion_result.get("extracted") if isinstance(ingestion_result, dict) else None
        except Exception as e:
            logger.exception("[handler] Local ingestion handler failed")
            ingestion_result = {"status": "error", "reason": str(e), "contract_id": contract_id}
            extracted = None
    else:
        logger.info("[handler] Local ingestion handler not available; skipping local ingestion call")
        extracted = None

    # Now call risk analysis handler locally using ingestion result
    risk_result = None
    if risk_main and hasattr(risk_main, "handler") and isinstance(ingestion_result, dict) and ingestion_result.get("status") == "INGESTED":
        try:
            logger.info("[handler] Invoking local risk analysis handler with ingestion output")
            risk_event = {
                "contract_id": contract_id,
                "extracted": ingestion_result.get("extracted"),
                "s3": sfn_input.get("s3"),
                "vendor_metadata": vendor_metadata
            }
            risk_result = risk_main.handler(risk_event, None)
            logger.info(f"[handler] Risk analysis returned keys: {list(risk_result.keys()) if isinstance(risk_result, dict) else 'N/A'}")
        except Exception as e:
            logger.exception("[handler] Local risk analysis handler failed")
            risk_result = {"status": "error", "reason": str(e), "contract_id": contract_id}
    else:
        logger.info("[handler] Local risk analysis handler not available or ingestion failed; skipping local risk call")

    # Start Step Functions execution
    execution_name = f"contract-{contract_id}-{uuid.uuid4().hex[:8]}"

    try:
        response = step_functions.start_execution(
            stateMachineArn=STATE_MACHINE_ARN,
            name=execution_name,
            input=json.dumps(sfn_input)
        )

        execution_arn = response["executionArn"]
        start_date = response["startDate"].isoformat()

        logger.info(f"[handler] Started Step Functions execution: {execution_name}")
        logger.info(f"[handler] Execution ARN: {execution_arn}")

        # Build a response object that includes contract_id and any ingestion/risk outputs
        result_payload = {
            "message": "Contract review workflow started",
            "execution_arn": execution_arn,
            "execution_name": execution_name,
            "contract_id": contract_id,
            "start_date": start_date,
            "s3_location": f"s3://{bucket}/{key}",
            "ingestion_result": ingestion_result,
            "risk_result": risk_result
        }

        return {
            "statusCode": 200,
            "body": json.dumps(result_payload),
            **result_payload
        }

    except Exception as e:
        logger.error(f"[handler] Failed to start Step Functions execution: {e}")

        error_payload = {
            "error": "Failed to start workflow",
            "details": str(e),
            "contract_id": contract_id,
            "ingestion_result": ingestion_result,
            "risk_result": risk_result
        }

        return {
            "statusCode": 500,
            "body": json.dumps(error_payload),
            **error_payload
        }



if __name__ == "__main__":
    # Local test event
    test_event = {
        "s3": {
            "bucket": "agentic-risk-automation-dev-artifacts",
            "key": "contracts/contract.pdf"
        },
        "vendor_metadata": {
            "region": "us-east-1",
            "contract_type": "MSA"
        }
    }

    result = handler(test_event, None)
    print(json.dumps(result, indent=2))
